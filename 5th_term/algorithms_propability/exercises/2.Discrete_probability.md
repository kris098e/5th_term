# good exercises (that you also understand)
7.4: 37, 30

# Chapter 7.1
![[Pasted image 20230918104147.png]]
$$\frac{4+13-1}{52}$$
4 aces, 12 cards left for red 
![[Pasted image 20230918104156.png]]
![[Pasted image 20230918104840.png]]
for all of the ways of picking an ace, there 48 cards left to choose from where we have to choose 4. 

**Remember** that we just have to find the number of ways the condition holds, and then divide by the entire probability space
![[Pasted image 20230918104205.png]]

we can first choose from 40 cards since king, queen and knight we cannot choose.

$$\frac{{40\choose 1}{4\choose 1}{4\choose 1}{4\choose 1}{4\choose 1}}{52\choose 5}$$
Since at first we can choose from 40 cards, next we have to choose 1 out of the 4 possible colours of the next valid card. We continue with this path

![[Pasted image 20230918110642.png]]

![[Pasted image 20230918104224.png]]
We just see how many possibilites there are of each and dividing by the sample space
for two dice: $$\frac{5}{6^6}$$
for three dice:
$$\frac{21}{6*6*6}$$

![[Pasted image 20230918104240.png]]
a.
![[Pasted image 20230918113120.png]]
We find the probability that we dont hit a 6 4 times, as these are independent events we just multiply them. Then we subtract this from 1, and get the probability of hitting a one at least once

b.
![[Pasted image 20230918113427.png]]
We do the same as before calculcating the probability of this not happening and find the probability of it happening by subtracting it from 1

c.
look at a! yes!

# 7.2
![[Pasted image 20230918113640.png]]
![[bools_inequality.excalidraw]]
On the left side, we calculate the probability by finding the size of unique elements given by the union and dividing by total number in the sample space.

Meaning on the left side we only count unique elements, on the right side, we overcount as we dont care about uniquness
## Formal proof / proof by induction
![[Pasted image 20230918155601.png]]
We cross it out since we know that it can only be between 0 and 1

![[Pasted image 20230918114132.png]]
Since E is calculated by unique elements, then we add the unique elements in F we can only get a probability which is >= max(p(E), p(F))

![[7.2_12E.excalidraw]]
The most amount of uniqure elemtns in F can possibly be 20% as E fills up 80%.

![[Pasted image 20230918114730.png]]
Probability of the first being being heads = $\frac{1}{2}$

head = 1
tails = 0

first being head: {1,0,0,0,0}
all possibilites is $2^5$ 

Intersection
{1,1,1,1,0}
{1,1,1,0,1}
{1,1,0,1,1}
{1,0,1,1,1}

$$\frac{\left( \frac{4}{2^5} \right)}{\frac{1}{2}}=\frac{1}{4}$$

$$\frac{\frac{{4\choose 3}}{2^5}}{0.5}$$
4 choose 3 because we can choose 3 of the last 4 to be heads and then we divide by the possibility throwing a heads for the first one

![[Pasted image 20230918115819.png]]
a.
the possibility of picking the string with only 1 bits is $$1-\left( \frac{1}{2^{10}} \right)$$
b.
success, pick a 1
failure pick a 0

The events of picking are independent, so we can use bernolli trials
$${10\choose 10}0.6^{10}*(1-0.4)^{10-10}$$
As we have `n=10` we pick `10` the possibility of picking the correct solution each time is `0.6`, and we need to do this 10 times. As these events are independent we multiply them together. As there are no restrictions for the some other elements (fx if we had to pick 10 out of 12) then we have not chance of the failures

c.
Since we have to choose 10 out of 10, the success of the first element is $\frac{1}{2}$ as the next element has $\frac{1}{2^2}$ of being a success and these are independent events, we multiply them to get the possibility that we have 2 1's in a row. We continue with this and get the proabability of the event is
![[Pasted image 20230918122202.png]]

![[Pasted image 20230925135829.png]]
![[Pasted image 20230925145040.png]]


![[Pasted image 20230925135841.png]]
a.
E=atleast 1x6
F=sum is 7
$$p(F|E)=\frac{p(E\cap F)}{p(E)}=\frac{\frac{2}{36}}{\frac{11}{36}}=\frac{2}{11}$$

$$P(E)=1-\left( \frac{5}{6} \right)^2=\frac{11}{36}$$
$$p(E\cap F)=\frac{2}{36}$$
b.
same as a.

# 7.3 Bayes thorem
![[Pasted image 20230922131208.png]]
a. since the probability of you choosing the correct door is 1/3, it would be 1/3
b. 
if j=k then the chance is 0
if j=i then the change is 0

if $i\neq j$, $i\neq k$, $j\neq k$ then the chance is 1
if $j\neq k$, $i=k$ then the chance is $\frac{1}{2}$

c.
$$p(W=k|m=j)=\frac{p(W=j|M=k)p(M=k)}{\sum_{l=\{1,2,3\}}p(W=j|M=l)p(M=l)}=\frac{1\frac{1}{3}}{\frac{1}{3}( 1+\frac{1}{2} +0)}=\frac{2}{3}$$
d.
`c.` explains the chance of choosing the winning door when it shifts
![[Pasted image 20230922131709.png]]
## Boxes
let E the event that Frida picks from box 1
let !E be the even that Frida pick from box 2
## Balls
let F be the event that Frida picks a blue ball
let !F be the even that Frida picks a red ball
## Probabilities
we want to find Pr(E|F)

Pr(F|E)=3/5, since 3/5 balls match in the 1st box
Pr(F|!E)=1/5, since 1/5 balls match in the 2nd box

Pr(F)=4/10 since 4 balls out of 10 total
Pr(!F)=6/10

we then insert and have
$$Pr(E|F)=\frac{Pr(F|E)*Pr(E)}{Pr(F|E)*Pr(E)+pr(F|!E)*Pr(!E)}=\frac{\frac{3}{5}*\left( \frac{1}{2} \right)}{\frac{3}{5}*\left( \frac{1}{2} \right)+\frac{1}{5}*\left( \frac{1}{2} \right)}=\frac{3}{4}$$
![[Pasted image 20230922131721.png]]
true positive is 98

### a) Find the probability that someone who tests negative for opium use does not use opium. 
$Pr(Positive \ | \ Clean)=0.02$
$Pr(Negativ \ | \ Clean)=1-0.02=0.98$
$Pr(Negative \ | \ Addict)=0.05$ 
$Pr(Positive \ | \ Addict)=1-0.05=0.95$ 
$Pr(Addict)=0.01$ $$ \begin{align*} Pr(Clean \ | \ Negative ) &= \frac{Pr(N \ | \ C)Pr(C)}{Pr(N \ | \ C)Pr(C)+Pr(N \ | \ A)Pr(A)} \\ &= \frac{0.98 \cdot 0.99}{0.98 \cdot 0.99 + 0.05 \cdot 0.01}\\ &= 0.99948 \end{align*}$$ ### b) Find the probability that someone who tests positive for opium use actually uses opium $$ \begin{align*} Pr(Addict \ | \ Positive)&= \frac{Pr(P | A)Pr(A)}{Pr(P | A)Pr(A) \cdot {Pr(P \ | \ C)}Pr(C)}\\ &=\frac{0.95 \cdot 0.01}{0.95 \cdot 0.01 + 0.02 \cdot 0.99}\\ &= 0.32423 \end{align*} $$
![[Pasted image 20230922131736.png]]
$I=0.04$ $\overline{I}=1-0.04=0.96$ $Pr(P \ | \ I)=0.97$ $Pr(N \ | \ I)=1-0.97=0.03$ $Pr(P \ | \ \overline{I})=0.02$ $Pr(N \ | \ \overline{I})=1-0.02=0.98$ ### a) a patient testing positive for avian influenza with this test is infected with it? $$ \begin{align*} Pr(I \ | \ P)&= \frac{Pr(P \ | \ I)Pr(I)}{Pr(P \ | \ I)Pr(I) \cdot Pr(P | \overline{I}) \cdot Pr(\overline{I})}\\ &= \frac{0.97 \cdot 0.04}{0.97 \cdot0.04 + 0.02 \cdot 0.96}\\ &= 0.66897 \end{align*}$$ ### b) a patient testing positive for avian influenza with this test is not infected with it? $$ \begin{align*} Pr(\overline{I} \ | \ P)&= \frac{Pr(P \ | \ \overline{I})Pr(\overline{I})}{Pr(P \ | \ \overline{I})Pr(\overline{I})+Pr(P \ | \ I)Pr(I)}\\ &= \frac{0.02 \cdot 0.96}{0.02 \cdot0.96+0.97 \cdot 0.04}\\ &= 0.33103 \end{align*} $$ Easier: $Pr(\overline{I} \ | \ P)=1-Pr(I \ | \ P)=1-0.66897=0.33103$ 
### c) a patient testing negative for avian influenza with this test is infected with it? $$ \begin{align*} Pr(I \ | \ N)&= \frac{Pr(N \ | \ I)Pr(I)}{Pr(N \ | \ I)Pr(I) \cdot Pr(N \ | \ \overline{I}) \cdot \overline{I}}\\ &= \frac{0.03 \cdot 0.04}{0.03 \cdot 0.04 + 0.98 \cdot 0.96}\\ &= 0.00127 \end{align*} $$ ### d) a patient testing negative for avian influenza with this test is not infected with it? $$Pr(\overline{I}\ | \ N)=1-Pr(I \ | \ N)=1-0.00127=0.99873$$

![[Pasted image 20230922131751.png]]
Use the expanded bayes theorem

![[Pasted image 20230922131839.png]]
![[Pasted image 20230922131848.png]]
# 7.4 expected values and variance
![[Pasted image 20230925130928.png]]
![[Pasted image 20230925161445.png]]

![[Pasted image 20230925131211.png]]

![[Pasted image 20230925131230.png]]
a. 
Largest = p(B) since have intersection
smallest = $$\frac{2}{3}-\frac{1}{2}$$
since we have one of the probabilities being > 0.5, we will always have some intersection

b.  
largest = 1 if they have as little commons as possible
smallest = the smallest i can be is if B is inside of A, then we dont gain anything by the union and end up with `p(A)`

![[Pasted image 20230925131257.png]]
$$p(B|A)=\frac{p(A\cap B)}{p(A)}$$
$$\frac{p(A\cap B)}{p(A)}<p(B)$$
![[Pasted image 20230925131315.png]]
chebyshevs tells us the probability of afvige with that much from the expectation.

it must afvige with a value of 10 in the inequality, since the expected value is 1, and it must afvige with more than 10.

probability of 2 different persons getting their hat back is $\frac{1}{n}* \frac{1}{n-1}$ since if one gets their hat correctly back, the probability of you getting your hat back correctly is now more likely

the number of ways to make $(1,2),(1,3)\dots (1,n)+(2,3)\dots (n-1,n)$ is $n*(n-1)$ Since we want to remove all included where it looks like $(i,i) |i\in\{1,2\dots n\}$

## In class
![[Pasted image 20231009084034.png]]

![[Pasted image 20230925131328.png]]
we have ${(m+n-1)\choose m-1}p^{m-1}q^n*p$ The extra `p` since we want it to appear on the `m+n`th trial
![[Pasted image 20231002160630.png]]
![[Pasted image 20230929080613.png]]
probability of random variable taking value bigger than a is less than the expected value divided by a

**intuition**: write down the expression, want to introduce a and round of some of them down
1. write up definition
2. split the cases, the cases where the random variable is more than a and the case where it is less than a
3. begin trying to throw away expressions to match the case
	1. In this case we replace `X(S)` with the bare minimum value it can take
	2. we can throw away the other expression since `x(S)` has to be nonnegative
4. we simplify, and since `a` is a constant we can put out of the sum
5. in the sum now we just sum of the probabilities that `X(S)>=a`.
6. Now if we divide by a on both sides, we get what we want (look in the chain)
![[Pasted image 20231009085242.png]]

![[Pasted image 20230929081714.png]]
Let `X=sum of eyes`.
The probability of getting each eye-combination then must be $\frac{1}{6^3}$
```python
sum = 0.0
probability = 1.0/(6**3)

for i in range(1, 7):
    for j in range(1,7):
        for k in range(1,7):
            sum += probability * (i + j + k)
print(sum) # 10.5
```
## another solution (better)
Can use linearity of expectation. 
I.e we have three die, the expected value of one of the die is `3.5` and we then just have
$$E(X)=E(X_{1}+X_{2}+X_{3})=E(X_{1})+E(X_{2})+E(X_{3})=3*3.5=10.5$$
![[Pasted image 20230929082555.png]]
a.
## Not correct
$\left( \frac{1}{6} \right)^n$
## Correct
Probability of not hitting a 6, and on the nth time we will want to roll a 6
$\left( \frac{5}{6} \right)^{n-1}\left( \frac{1}{6} \right)$


b.
It is 6, since the probability is $\frac{1}{6}$
but for the sake of notation, we use an indicator variable, and let `X=1` when the throw is a 6.

this means we have the terms
$$E(X_{throw\ 1} +X_{throw\ 2\dots + X_{throw\ n}})= E(X_{throw\ 1})+E(X_{throw\ 2})\dots=\frac{n*1}{6}=1, n$$ $$n=6$$
## Could also do (more correct)
more correct since we may not actually throw fx 4 times, if we throw a 6 on the 3rd time.

random variable: number of roles before getting a 6
$$E(X)=\frac{1}{6}+2\left( \frac{5}{6} \right)^1\left( \frac{1}{6} \right)+3\left( \frac{5}{6} \right)^{2}\left( \frac{1}{6} \right)\dots i\left( \frac{5}{6} \right)^{i-1}\left( \frac{1}{6} \right)=\frac{1}{\left( \frac{1}{6} \right)}=6$$

![[Pasted image 20230929085259.png]]
## Wrong
Again we use the the random variable as an indicator variable, meaning `p(X=1)=1/6`.
Now the expectation is as before, where we have **10 throws** **now instead of 6**. Meaning
$$E(all\ throws)=\frac{throws}{6}=\frac{10}{6}$$
variance is
$$E(X^2)-E(X)^2$$
meaning
$$E(X)^2=\left( \frac{10}{6} \right)^2$$
and
$$E(X^2)=10*\left( \frac{1}{6} \right)$$
conclusion
$$Var(X)=10*\left( \frac{1}{6} \right)-\left( \frac{10}{6} \right)^2$$
## Correct
As we use indicator random variables, we can just use the binomial distribution formula for the variance:
$$Var(X) = n * p * (1 - p) = 10 * (1/6) * (5/6)=50/36 = 25/18.$$
## Explanation in a more written out case
![[Pasted image 20231009101524.png]]
It is easy to calculate $E(X)^{2}$ since we easily can calculate $E(X)=\frac{10}{6}$. 
**but look at what $E(X^2)$ expands to**. This is because we have all combinations of the random variables. 
### Example $E(X_{1}^2)$
will take the value `1` if and only if both $X_{1}$'s take the value we define. Now as this is the same, it will still be $\frac{1}{6}$
### Example $E(X_{1},X_{2})$
This will take the value `1` if both are satisfied. Meaning that, since these events are independant from each other, this will take the value `1` with the probability $\frac{1}{6}* \frac{1}{6}=\frac{1}{36}$

![[Pasted image 20230929091924.png]]
a.
Markovs inequality. 
![[Pasted image 20230929092513.png]]
then it is
$$p(X(s)\geq 11000)=\frac{10000}{11000}=\frac{10}{11}$$
b.
chebyshevs inequality
![[Pasted image 20230921131210.png]]
we use $r=1000$ since we want to see what the probability that we afviger with `MORE` than 1000

$$p(\mid X(S)-E(X)\mid\geq r)\leq\frac{V(X)}{r^2}$$
insert
$$P(\mid X(S)-10000\mid\geq r)\leq \frac{1000}{1000^2}=\frac{1}{1000}$$

now we calculate the probability we hit inside of the interval
$$P(\mid X(S)-E(X)\mid<1000)=1-\frac{1}{1000}=\frac{999}{1000}$$

![[Pasted image 20230929093040.png]]
The expected number of balls can be calculated using an indicator random variable, having `m` balls, and the probability of getting the first bin is $\frac{1}{n}$, then when we have `m` trials, it is
$$\frac{m*1}{n}$$
## Formal
we use random indicator variables, and for each ball we have a random indicator variable which takes the value `1` if the ball hits the specific ball. We then use `linerarity of expectation` and see we get what is written above.

![[Pasted image 20230929093522.png]]
p(forudsiger|regn)=0.9
p(forudset|!regn)=0.1

p(regn)=$\frac{5}{365}$
![[Pasted image 20230917160450.png]]
$p(Regn|forudser)=\frac{p(forudser|regn)p(regn)}{p(forudser|regn)p(regn)+ p(forudset|!regn)p(regn)}=\frac{\frac{0.9 \cdot5}{365}}{\frac{0.9*5}{365}+\frac{0.1*360}{365}}$
![[Pasted image 20230929095316.png]]

![[Pasted image 20231007093651.png]]
a.
since the mappings are independent from eachother, the probability of it being 1-1 and onto (which it will be since it is 1-1 and the set of S is equally as big as T) is then
$$\Pi_{i=0}^{n-1}\frac{n-i}{n}=\frac{n!}{n^n}$$
since the first can hit anything. So deviating from the solution has the chance 0. In other words, The chance of hitting a correct element is `1`


b. 
Since S and T are equally big, and the probability of something from S being mapped to something in T is $\frac{1}{n}$ where $n$ is the size of T and S, then we let a random variable equal 1 if the element $T_{j}$ is mapped to, where $j=\{1,2\dots\mid T\mid\}$ and 0 otherwise. Since these are independent, we expect the random variable to be 1 with $\frac{1}{n}$ chance, meaning the expected value of the specific random variable $X_{j}$ is $\frac{1}{n}$. We then define this random variable for all possible values for $j$, where we can use the linearity of expectation meaning:
$$E(X_{0}+X_{1}+\dots+X_{j})=E\left( \sum_{j=0}^{n}X_{j}\right)=\sum_{j=0}^{n} E(X_{j})=\frac{n*1}{n}=1=\mid f^{-1}(t)\mid$$

could also use bernolli trials where we have $n \cdot p$

c.
lad $s\in S$ what is the probability that $p(s\to t)$ and no other element does. I.e the probability that something from S goes to t, and no other elements does

There are n ways to pick the element s.

the probability that all of the other s does not hit the element t is $\left( \frac{n-1}{n} \right)^{n-1}$ since they should not hit it.
remember we fix s and t.

d. 
s mapped to t again, and now we have the same question as before, but with `n-1` elements 
$\left( \frac{n-2}{n-1} \right)^{n-2}$

e. 
they are dependant, as we can see it has effect on the calculation, that we first want the 1st calculation to happen, or that s maps to t, and then we do it again

![[Pasted image 20231013133629.png]]
we see that when we isolate in the formula for probability with dependency, `see the crossed out terms`, that it the dependable probability should be $p(U_{t})$ which it is not. See that there is difference in c and d.

![[Pasted image 20231013134446.png]]
$x_{t}$ is number of time $U_{t}$ happens, over the `r` experiments
make a indicator random variable, which is 1 when $U_{t}$ happens, else 0.
the expected value for this random indicator variable is then the probability for $U_{t}=\left( \frac{n-1}{n} \right)^{n-1}$

We have to do it for all `n` possible fixations on a specific `t` for a given `s`, so we do it `n` times, since we have `n` random indicator variables, one for each of the `t` the `s` can be mapped to.

![[Pasted image 20231013133735.png]]
#chernoff_bounds chernoff bounds
put delta = 1, since we want $2$ times the expected value. when 
![[Pasted image 20231013135934.png]]


![[Pasted image 20231007095549.png]]


![[Pasted image 20231007095709.png]]
## a
This is the coupon collector problem
$$nH(n)$$
### Explanation
see coupon collector problem
## b
not just $2 \cdot nH(n)$

![[Pasted image 20231007095745.png]]
- Just color each with a random color of 1...4
- if we fix on a node, and choose a coloring. 
	- probability of all of its neighbours having chosen a different color is $\frac{3}{4}$. And worst case, this has to happen $3$-times. I.e probability of success is bounded by $\left( \frac{3}{4} \right)^3$
		- k is at max 3
- make a random indicator variable, 
	- 1 if good
	- 0 if not
- Use linearity of expectation
	- $E(X)=\sum_{v\in V}\geq \sum_{v \in V}\left( \frac{3}{4} \right)^3=n\left( \frac{3}{4} \right)^3={n{\frac{27}{64}}}$
