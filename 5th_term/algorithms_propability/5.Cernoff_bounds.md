[see video here](https://sdu.itslearning.com/ContentArea/ContentArea.aspx?LocationID=29138&LocationType=1#)
# chatten
  
The Chernoff bound is a mathematical inequality used in probability theory and statistics to estimate the probability that the sum of independent random variables deviates significantly from its expected value. It provides an upper bound on the probability of such deviations, which is useful in various applications, including analyzing the performance of algorithms, the behavior of random processes, and error probabilities in information theory and coding theory. The bound is named after Herman Chernoff, who introduced it in the 1950s.

## own notes to chatten
if we fx have experiments of throwing heads/tails, then if one of the indicator random variables grows very large, since we got heads so many times (which is very unlikely), then another coin may have been more unlucky with getting the heads. So they should cancel out if we sum them up.
## Thoery
States that the probability that the sum of all the expected values for the random variables taking value more than `1+sign*(some variable)` is bounded
![[Pasted image 20231012134853.png]]
![[Pasted image 20231012134812.png]]
