# Shared resource
$P(S_{i,t})=p(1-p)^{n-1}$ since we want process `i` to get attempt, and all of the other n processes does not attempt. We can do this since we are doing the intersection
![[Pasted image 20231003122934.png]]
When is the probability then maximized? We say now that $f(p)=p(1-p)^{n-1}$, and we can then `differentiate` it and set it to 0 and find the `maxima`.

the maxima is then $\frac{1}{n}$. This makes sense since if we do random indicator variables, the expected number of processes which tries are 1 when the probability for each is $\frac{1}{n}$. The probability of not getting in with the optimal probability is then $1-f\left( \frac{1}{n} \right)$. We can insert into the formula for the function, and we get $$1-\left( \frac{1}{n} \right)\left( 1-\frac{1}{n} \right)^{n-1}$$
Since the formula we get for not getting in is hard to work with, so we simplify it, using the convergence
![[Pasted image 20231003123453.png]]
We substitute it and do it with bounds

# min-cut / karger's algorithm
[https://www.youtube.com/watch?v=bhryNCk5pn0](https://www.youtube.com/watch?v=bhryNCk5pn0)
select 1 of the n edges in total
at the end of the program we return how many edges are left between the two super vertices. But we **group** the edges such that if there are `10` edges between two verticies we will effectively remove 10 of them at the same time.
## Number of min cuts
is at most $n\choose 2$. Think of this as the fact that these two chosen vertices cannot be in the same disjoint component from the two components formed by the min-cut.
## Recap
It is all about looking into the probability of choosing to contract two vertecies, which will one of the edges which is part of the mincut. We look into if we do it on the first contraction, 2nd contraction ... nth-2 contraction. this is the probability that we do return the min cut.
# 13.3 Random variables and their expectation, collecting coupons
have an example of how many boxes we expect to have to buy before we get all of the `n` coupons, where each box contains exactly 1 coupon. Now, we define random variables, where
1. first random variable indicates getting 1 coupon we we havent seen yet, and since we have not seen any yet, this is $\frac{n}{n}=1$
2. 2nd random variables indicates the same, but we have gotten 1 of the coupons already, so this is $\frac{n-1}{n}$ chance. We expect to try $\frac{1}{\left( \frac{n-1}{n} \right)}=\frac{n}{n-1}$ times for this to happen
3. this continues down to $\frac{0}{n}$.
4. we end up with, we expect to buy $\sum_{j=0}^{n} \frac{n}{n-j}=\sum_{k}^{n}n \frac{1}{k}=nH(n)=O(n \ln n)$ boxes, as $H(n)=\ln n$ approximately.
# 13.4 A Randomized Approximation Algorithm for MAX 3-SAT
**remember to write out the calculation during the exam**
- 3-SAT means that we have `n`-clauses with 3 variables inside. Now there are $\left( \frac{1}{2} \right)^3$ ways to not satisfy a clause, as all these has to not be true. This means that the probability of a clause being satisfied is $1-\left( \frac{1}{2} \right)^3=\frac{7}{8}$. Now how many clauses are expected to be satisfied if we have `n`-clauses? We use a random indicator variable for this. So the expected number of clauses which are satisfied is $\sum_{i=0}^{n} \frac{7}{8}=n \frac{7}{8}$.
	- But the expected number of clauses not satisfied can also be calculated by looking a random variable which is 1 if the clause is not satisfied. $E(X_{i})=\frac{1}{2^{k}}$.
		- Now if $E\left( \sum\limits_{i=1}^{m}X_{i} \right)=m \frac{1}{2^{k}}$. So as long as $m < 2^{k}$ then $E(X) < 1$. Meaning the expected number of clauses not satisfied is less than 1.
## Waiting to Find a Good Assignment
See the book
## For k-sat
we have $\frac{2^k-1}{2^k}m$ satisfied clauses satisfied. Fx see if we use `k=3`.