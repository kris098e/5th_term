# Shared resource
$P(S_{i,t})=p(1-p)^{n-1}$ since we want process i to get attempt, and all of the other n processes does not attempt. We can do this since we are doing the intersection
![[Pasted image 20231003122934.png]]

When is the probability then maximized? We say now that $f(p)=p(1-p)^{n-1}$, and we can then `differentiate` it and set it to 0 and find the `maxima`.

the maxima is then $\frac{1}{n}$. This makes sense since if we do random indicator variables, the expected number of processes which tries are 1 when the probability for each is $\frac{1}{n}$. The probability of not getting in with the optimal probability is then $1-f\left( \frac{1}{n} \right)$. We can insert into the formula for the function, and we get $$1-\left( \frac{1}{n} \right)\left( 1-\frac{1}{n} \right)^{n-1}$$
Since the formula we get for not getting in is hard to work with, so we simplify it, using the convergence
![[Pasted image 20231003123453.png]]
We substitute it and do it with bounds

# min-cut / karger's algorithm
[https://www.youtube.com/watch?v=bhryNCk5pn0](https://www.youtube.com/watch?v=bhryNCk5pn0)
select 1 of the n edges in total
at the end of the program we return how many edges are left between the two super vertices. But we **group** the edges such that if there are `10` edges between two verticies we will effectively remove 10 of them at the same time.
## Number of min cuts
is at most $n\choose 2$

# 13.3 Random variables and their expectation, collecting coupons
have an example of how many boxes we expect to have to buy before we get all of the `n` coupons, where each box contains exactly 1 coupon. Now, we define random variables, where
1. first random variable indicates getting 1 coupon we we havent seen yet, and since we have not seen any yet, this is $\frac{n}{n}=1$
2. 2nd random variables indicates the same, but we have gotten 1 of the coupons already, so this is $\frac{n-1}{n}$ chance. We expect to try $\frac{1}{\left( \frac{n-1}{n} \right)}=\frac{n}{n-1}$ times for this to happen
3. this continues down to $\frac{0}{n}$. 
4. we end up with, we expect to buy $\sum_{j=0}^{n} \frac{n}{n-j}=\sum_{k}^{n} \frac{1}{k}=nH(n)=O(n \ln n)$ boxes, as $H(n)=\ln n$ approximately. 

# 13.4 A Randomized Approximation Algorithm for MAX 3-SAT
**remember to write out the calculation during the exam**
- 3-SAT means that we have `n`-clauses with 3 variables inside. Now there are $\left( \frac{1}{2} \right)^3$ ways to not satisfy a clause, as all these has to not be true. This means that the probability of a clause being satisfied is $1-\left( \frac{1}{2} \right)^3=\frac{7}{8}$. Now how many clauses are expected to be satisfied if we have `n`-clauses? We use a random indicator variable for this. So the expected number of clauses which are satisfied is $\sum_{i=0}^{n} \frac{7}{8}=n \frac{7}{8}$.
		- Now since one of the assignments to the `k` possible variables, $x_{1}, x_{2}\dots x_{k}$ must satisfy more than or equal to the expected number of clauses satisfied, and the number of clauses must be an integer, we have that $\lceil n \frac{7}{8}\rceil=n$
- 
## Waiting to Find a Good Assignment
See the book
## For k-sat
we have $\frac{2^k-1}{2^k}m$ satisfied clauses satisfied. Fx see if we use `k=3`.